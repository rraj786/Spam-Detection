---
title: "Spam Detection"
output:
  word_document: default
  html_notebook: default
author: Rohit Rajagopal
---

### The following script utilises the SMS Spam Detection dataset from UCI to build various predictive models to categorise whether a message is spam or not. 

```{r}
# Load required libraries
library(tidyverse)
library(rpart)
library(e1071)
library(ranger)
library(textstem)
library(tm)
library(ggplot2)
library(wordcloud)
set.seed(10)
```

**Data Loading**

Read in CSV file and modify dataset as required.

```{r}
# Read in dataset
sms_data <- read_csv("spam.csv")

# Find dataset dimensions
cat("Shape of Spam Dataset:", paste(dim(sms_data), collapse = "x"), "\n")
sms_data
```

There seem to be a few blank columns, let's check these don't contain any values at all.

```{r}
# Count non-blank rows in each column using !is.na()
non_na_counts <- colSums(sapply(sms_data, function(x) !is.na(x)))
print(non_na_counts)
```
All the columns contain non-NA values, therefore let's concatenate columns 2 to 5 to form the entire message.

```{r}
# Create combined column and drop the remaining ones
sms_data$v2 <- paste(sms_data$v2,
                       ifelse(sms_data$...3 != "" & !is.na(sms_data$...3), sms_data$...3, ""),
                       ifelse(sms_data$...4 != "" & !is.na(sms_data$...4), sms_data$...4, ""),
                       ifelse(sms_data$...5 != "" & !is.na(sms_data$...5), sms_data$...5, ""),
                       sep = " ")
sms_data <- sms_data[, -c(3, 4, 5)]

# Rename remaining columns for readability
colnames(sms_data) <- c("spam_label", "text")
sms_data
```

**Data Visualisation**

Let's have a look at how the dataset is distributed.

```{r}
# Find number of spam and non-spam messages
counts <- table(sms_data$spam_label)
print(counts)
```

Evidently, there are much more ham (non-spam) messages than spam in the dataset. This leads to extreme class imbalance and may affect the reliability of the models.

```{r}
# Check and convert text encoding UTF-8
sms_data$text <- iconv(sms_data$text, to = "UTF-8", sub = "byte") 

# Find number of characters in text
char_lengths <- nchar(sms_data$text)

# Create a histogram of characters in each message
ggplot(sms_data, aes(x = char_lengths, fill = spam_label)) +
  geom_histogram(binwidth = 10, position = "identity", alpha = 0.5, color = "black") +
  labs(title = "Histogram of Character Lengths by Spam Status",
       x = "Character Lengths",
       y = "Frequency") +
  scale_fill_manual(values = c("lightblue", "salmon")) +
  theme_minimal()

# Get summary stats
print(tapply(char_lengths, sms_data$spam_label, summary))
```

The plot above indicates that spam messages usually have more characters than ham, with a peak at around 150 compared to the latter at 25. The summary stats support this view.

```{r}
# Find number of words in text
word_lengths <- str_count(sms_data$text, "\\S+")

# Create a histogram of words in each message
ggplot(sms_data, aes(x = word_lengths, fill = spam_label)) +
  geom_histogram(binwidth = 5, position = "identity", alpha = 0.5, color = "black") +
  labs(title = "Histogram of Word Lengths by Spam Status",
       x = "Word Lengths",
       y = "Frequency") +
  scale_fill_manual(values = c("lightblue", "salmon")) + 
  theme_minimal()

# Get summary stats
print(tapply(word_lengths, sms_data$spam_label, summary))
```

The plot above once again dictates that spam messages are likely to contain more words than their ham counterparts. The former has a mean of 24 compared to 14 for the latter.

```{r}
# Find whether each message contains numbers and if so, how many
has_numbers <- grepl("[[:digit:]]", sms_data$text)
number_count <- sapply(gregexpr("[[:digit:]]", sms_data$text), function(x) sum(x > 0))

# Split of spam and non-spam messages containing numbers
number_count_splits = table(sms_data$spam_label, has_numbers)
cat("\n")
print(number_count_splits)

# Create a histogram of the occurrences of numbers in each message
ggplot(sms_data, aes(x = number_count, fill = spam_label)) +
  geom_histogram(binwidth = 3, position = "identity", alpha = 0.5, color = "black") +
  labs(title = "Histogram of Number by Spam Status",
       x = "Number Occurrences",
       y = "Frequency") +
  scale_fill_manual(values = c("lightblue", "salmon")) + 
  theme_minimal()

# Get summary stats
cat("\n")
print(tapply(has_numbers, sms_data$spam_label, summary))
cat("\n")
print(tapply(number_count, sms_data$spam_label, summary))
```

Spam messages seem to contain more numbers than ham. This likely pertains to the fact that spam messages usually contain phone numbers to dial or strange links to click on.

```{r}
# Find whether each message contains non-alphanumeric characters and if so, how many
has_non_alphanum <- grepl("[^[:alnum:]]", sms_data$text)
non_alphanum_count <- sapply(gregexpr("[^[:alnum:]]", sms_data$text), function(x) sum(x > 0))

# Split of spam and non-spam messages containing non-alphanumeric characters
non_alphanum_count_splits = table(sms_data$spam_label, has_non_alphanum)
cat("\n")
print(non_alphanum_count_splits)

# Create a histogram of the occurrences of non-alphanumeric characters in each message
ggplot(sms_data, aes(x = non_alphanum_count, fill = spam_label)) +
  geom_histogram(binwidth = 5, position = "identity", alpha = 0.5, color = "black") +
  labs(title = "Histogram of Non-alphanumeric Character Occurrences by Spam Status",
       x = "Non-alphanumeric Character Occurrences",
       y = "Frequency") +
  scale_fill_manual(values = c("lightblue", "salmon")) + 
  theme_minimal()

# Get summary stats
cat("\n")
print(tapply(has_non_alphanum, sms_data$spam_label, summary))
cat("\n")
print(tapply(non_alphanum_count, sms_data$spam_label, summary))
```

Finally, spam messages also have more non-alphanumeric characters (punctuation, emoticons etc.) than ham. All 4 plots above suggest there is a distinct separation between the two groups which may be useful for the model building process.

**Data Preprocessing**

Clean and process text data using a standardised NLP approach (such as removing punctuation, stop words, and lemmatisation), such that the models can interpret each message correctly.

```{r}
# Create corpus of words
corpus <- Corpus(VectorSource(sms_data$text))

# Convert to lower case
corpus <- tm_map(corpus, content_transformer(tolower))

# Remove punctuation
corpus <- tm_map(corpus, content_transformer(str_replace_all), pattern = "[[:punct:]]", replacement = " ")

# Remove numbers
corpus <- tm_map(corpus, content_transformer(str_replace_all), pattern = "[[:digit:]]", replacement = " ")

# Remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))

# Perform lemmatisation (reduce words to their canonical or dictionary form in English)
corpus <- tm_map(corpus, lemmatize_strings)

# Remove extra white spaces
corpus <- tm_map(corpus, content_transformer(str_replace_all), pattern = "\\s+", replacement = " ")
```

```{r}
# View first 10 messages in corpus to see cleaned output
for (i in 1:10) {
  line <- corpus[[i]][["content"]]
  cat(line, "\n")
}
```

```{r}
# Create a Term-Document Matrix (TDM)
tdm <- DocumentTermMatrix(corpus)
tdm_matrix = as.matrix(tdm)
spam_matrix <- tdm_matrix[which(sms_data$spam_label == "spam"), ]
ham_matrix <- tdm_matrix[which(sms_data$spam_label == "ham"), ]

# Convert dtm to a matrix of word frequencies
word_freq <- colSums(tdm_matrix)
spam_freq <- colSums(spam_matrix)
ham_freq  <- colSums(ham_matrix)

# Generate word clouds for spam and non-spam messages
spam_messages <- head(spam_freq, 75)
par(mfrow = c(1, 1)) 
wordcloud(names(spam_messages), spam_messages, min.freq = 1, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
title(main = "Word Cloud for Top 75 Words used in Spam Messages")

ham_messages <- head(ham_freq, 75)
par(mfrow = c(1, 1)) 
wordcloud(names(ham_messages), ham_messages, min.freq = 1, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
title(main = "Word Cloud for Top 75 Words used in Ham (Non-spam) Messages")
```

The word clouds above show that spam messages are likely to contain words such as "text", "send", "free", and "now" as they usually involve a transfer of private information. Ham messages on the other hand, contain more conversational phrases as expected.

```{r}
# Convert TDM to Dataframe
tdm_df <- as.data.frame(tdm_matrix, stringsAsFactors = FALSE)

# Append on additional features such as character and word count, numbers, and non-alphanumeric characters
tdm_complete = cbind("spam_label" = as.factor(sms_data$spam_label), char_lengths, word_lengths, non_alphanum_count, number_count, tdm_df)

# Find matrix dimensions
cat("Shape of Feature Matrix:", paste(dim(tdm_complete), collapse = "x"), "\n")
```

**Model Creation**

Build 5 different models to predict whether a message should be classified as spam or not, and then evaluate their performance metrics.

```{r}
# Split data into train and test sets using a 80/20 split
colnames(tdm_complete) <- c("spam_label", paste0("feature_", 1:(ncol(tdm_complete) - 1))) # Adjust column names for models
indices <- sample(nrow(tdm_complete), round(0.8 * nrow(tdm_complete)))
train <- as.data.frame(tdm_complete[indices, ])
test <- as.data.frame(tdm_complete[-indices, ])
```

Train models on train set, and predict on test set. Evaluate each of their performances.

```{r}
# Train, predict, and evaluate Naive Bayes model
nb_model <- naiveBayes(spam_label ~ ., data = train)
nb_pred <- predict(nb_model,  test[, -1])
confusionMatrix(nb_pred, test$spam_label)
```

The Naive Bayes classifier demonstrated poor performance with an accuracy of 11.94%. It failed to correctly identify any spam messages and misclassified 981 ham messages as spam. The classifier marginally outperformed the No Information Rate (88.06%) but showed no agreement beyond chance (Kappa = 0). Sensitivity (0%) and specificity (100%) were not informative due to the lack of true positives and true negatives, respectively. These metrics collectively indicate the classifier's inadequacy, highlighting the need for significant improvement to achieve meaningful predictive performance.

```{r}
# Train, predict, and evaluate Decision Tree model
tree_model <- rpart(spam_label ~ ., data = train, method = "class")
tree_pred <- predict(tree_model,  test[, -1], type = "class")
confusionMatrix(tree_pred, test$spam_label)
```

The Decision Tree model achieved an impressive accuracy rate of 97.31%, correctly categorising 962 ham messages and 122 spam messages. It encountered a challenge in misclassifying 19 ham messages as spam. Beyond simply exceeding the No Information Rate (88.06%), the model's Kappa coefficient of 0.8752 signifies a substantial level of agreement beyond chance. Notably, its sensitivity of 98.06% and specificity of 91.73% underscore its adeptness in identifying both spam and ham messages, respectively. Additionally, with a positive predictive value of 98.87% and a perfect negative predictive value of 86.52%, the model consistently demonstrated its reliability in predicting message classifications.

```{r}
# Train, predict, and evaluate  Random Forest model
rf_model <- ranger(spam_label ~ ., data = train, num.trees = 100, mtry = sqrt(ncol(train) - 1))
rf_pred <- predict(rf_model,  test[, -1])$predictions
confusionMatrix(rf_pred, test$spam_label)
```

The Random Forest classifier achieved a commendable accuracy rate of 98.83%, accurately identifying 981 ham messages and 120 spam messages. Only 13 spam messages were misclassified, showcasing its strong performance. Surpassing the No Information Rate (88.06%) with a Kappa value of 0.9421, the model substantiates a robust level of agreement beyond chance. Notably, with sensitivity at 100% and specificity at 90.23%, the classifier effectively differentiated between ham and spam messages. Further validation came from its positive predictive value of 98.69% and negative predictive value of 100%, reaffirming its precision in predicting message categories.

```{r}
# Train, predict, and evaluate  SVM model (Linear kernel)
svm_model <- svm(spam_label ~ ., data = train, scale = FALSE, kernel = 'linear')
svm_pred <- predict(svm_model,  test[, -1])
confusionMatrix(svm_pred, test$spam_label)
```

The SVM (linear kernel) delivered an outstanding accuracy of 99.28%, accurately classifying 976 ham messages and 130 spam messages. It encountered minimal misclassification, with only 3 ham messages and 5 spam messages inaccurately labeled. Demonstrating superior performance over the No Information Rate (88.06%) with a Kappa coefficient of 0.9661, the model achieved significant agreement beyond chance. Its sensitivity and specificity were robust at 99.49% and 97.74%, respectively, illustrating its effectiveness in distinguishing between ham and spam messages. Moreover, with a positive predictive value of 99.69% and a negative predictive value of 96.30%, the SVM's accuracy in predicting message categories was notably high, making it the most comprehensive model across all evaluated metrics.

**Results**

Apart from the Naive Bayes Classifier, the other models performed extremely well on the SMS Spam Detection dataset. SVM (linear kernel) was the best model across both spam and ham messages. 

However, the dataset was collected from messages dating back to 2006 and 2007 in the UK, Singapore, and the US, and may not accurately reflect current classifications due to changes in messaging habits and vocabulary. Initial data extraction relied on user reports from a website, introducing self-selection bias with potentially exaggerated accuracy in spam classification. The data's limited geographic scope and regional speech patterns hinder its global applicability for spam detection. Therefore, using this dataset alone for developing a spam detection model lacks the necessary diversity and broader representation needed for reliable results across different populations. To improve the generalisability of the models above, it is pertinent that a larger and more robust dataset is compiled.
